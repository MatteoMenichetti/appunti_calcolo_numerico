\section{Esercitazione capitolo 3}
\subsection{A.A. 2022/23}
\begin{enumerate}
    \item  Scrivere una function Matlab che risolva efficientemente un sistema triangolare inferiore.
    \item Definire la fattorizzazione $LU$ di una matrice nonsingolare. Dimostrare che, se fattorizzabile, la fattorizzazione $LU$ di una matrice è unica.
    \item  Sotto quali condizioni esiste la fattorizzazione $LU$ di una matrice?
    \item Definire cosa si intende per matrice a diagonale dominante. Dimostrare che una matrice diagonale dominante è fattorizzabile $LU$.
    \item  Definire cosa si intende per matrice simmetrica e definita positiva (sdp). Dimostrare che una matrice \textit{sdp} è fattorizzabile $LU$ .
    \item Dimostrare che una matrice simmetrica e definita positiva è fattorizzabile nella forma $LDL^T$, con $L$ triangolare inferiore a diagonale unitaria, e $D$ matrice diagonale.
    \item  Definire cosa si intende per norma indotta su matrice. Dare qualche esempio di tali norme.
    \item  Definire cosa è il numero di condizionamento di una matrice. Spiegarne il significato.
    \item Definire la soluzione nel senso dei minimi quadrati di un sistema lineare sovra-determinato a rango pieno. Dimostrarne l’esistenza ed unicità.
    \item Calcolare la matrice elementare di Householder $H$ relativa al vettore: 
    \begin{equation*}
       \boldsymbol z = \begin{pmatrix}
        -1\\
        2\\
        -2
    \end{pmatrix}.
    \end{equation*}
    Quanto vale il prodotto $H\boldsymbol z$?
    \item Definire il metodo di Newton per sistemi nonlineari, e dettagliarne l’implementazione.
\end{enumerate}

\paragraph{1.}\footnote{Slide 3 PDF 14.} La function è implementata nell'Algoritmo \ref{alg:trilow}.

\paragraph{2.}\footnote{Slide 3 PDF 14.} Sia $A\in\mathbb R^{n\times n},\, \det(A).$

\noindent (Risposta alla prima parte di domanda:) $A$ è fattorizzabile $LU$, se $A=LU$, con:
\begin{itemize}
    \item $L$ triangolare inferiore a diagonale unitaria:
    \item $U$ triangolare superiore.
\end{itemize}

\noindent (Risposta alla seconda parte) Unicità della fattorizzazione: È necessario dimostrare che se $A=L_1U_1$ è un'ulteriore fattorizzazione $LU$ di $A$, allora $L=L_1,\, U=U_1.$ Infatti:
\begin{equation*}
    0\neq\det(A)=\det(LU)=\underbrace{\det(L)}_{\footnotemark}\det(U)=\det(U).
\end{equation*}
\footnotetext{Uguale ad 1 perché a diagonale unitaria 1.}

\noindent Pertanto, da $LU=L_1U_1$, segue che, moltiplicando a destra per $U^{-1},\, L=L_1U_1U^{-1}.$ Da questa, moltiplicando a sinistra per $L_1^{-1},$ segue che:
\begin{equation}\label{eq:LL=UU}
    L_1^{-1}L=U_1U^{-1}.
\end{equation}

\noindent È possibile osservare ciò che segue:
\begin{itemize}
    \item $L_1^{-1}$ è una matrice triangolare inferiore a diagonale unitaria;
    \item  $U^{-1}$ è una matrice triangolare superiore.
\end{itemize}

\noindent Quindi $L_1^{-1}L$ è una matrice triangolare inferiore a diagonale unitaria e $U_1U^{-1}$ è una matrice triangolare superiore. Pertanto, le due matrici in (\ref{eq:LL=UU}) sono diagonali. Poichè la diagonale di $L_1^{-1}L$ è unitaria, questa è la matrice indentità. Dalle uguaglianze allora 
\begin{itemize}
    \item $L_1^{-1}L=I$,
    \item $U_1U^{-1}=I$,
\end{itemize}
segue che $L=L_1\wedge U_1=U$, ovvero la fattorizzazione $LU$ è unica. \qed

\paragraph{IMPORTANTE:} I passaggi come quelli sopra è necessario che siano il piu' dettagliati possibile affinché Luigi sappia che abbiamo studiato.

\paragraph{3.} Sia $A\in\mathbb R^{n\times n}, \det(A)\neq 0.$ Denotando con $A_k$ la sottomatrice principale di ordine $k$ di $A$, allora:
\begin{equation*}
    A=LU\iff\forall k=1,\hdots, n\, :\,\det(A_k)\neq 0.
\end{equation*}
Questa ultima riga non è sufficiente, è necessario specificare $A\in\mathbb R^{n\times n}.$

\paragraph{4.}\footnote{Slide 6 PDF 14.} Sia $\boldsymbol{A\in\mathbb R^{n\times n}}$. Allora $A=(a_{ij})$ è:
\begin{itemize}
    \item \textbf{diagonale dominante per righe}, se
    \begin{equation}\label{eq:ddRighe}
        |a_{ii}|>\sum_{j\neq i, j=1}^n|a_{ij}|,\quad\forall i=1,\hdots,n;
    \end{equation}
    \item \textbf{diagonale dominante per colonne}, se $|a_{jj}|>\sum_{i\neq j, i=1}^n |a_{ij}|,\;\forall j=1,\hdots,n.$
\end{itemize}
Valgono le seguenti proprietà:
\begin{itemize}
    \item[\textbf{P1)}] $A$ è d.d. per righe $\iff A^T$ è d.d. per colonne;
    \item[\textbf{P2)}] Se $A$ è d.d. per righe (o colonne), allora, detta $A_k$ la sottomatrice principale di ordine $k$ di $A$, essa sarà d.d. per righe (o colonne). (Dimostrazione:) Infatti, dato un generico $k\in\{1,\hdots,n\},$ risulterà che dalla (\ref{eq:ddRighe}) segue che $\forall i=1\hdots,k:$
    \begin{equation*}
         |a_{ii}|>\sum_{j\neq i, j=1}^n|a_{ij}|\geq \sum_{j\neq i, j=1}^k|a_{ij}|,
    \end{equation*}
    ovvero, $A_k$ è diagonale d.d. per righe. Analogamente per $A$ d.d. per colonne.\qed
    \item[\textbf{P3)}] Se $A$ è d.d. per righe (o per colonne), allora $A$ è nonsingolare. Infatti, se $A$ fosse \gls{singolare} esisterebbe $\uline{x}\in\mathbb R^n,\, x\neq 0\,:\, A\uline{x}=\uline 0$. Questa equazione vale per ogni multiplo di $\uline x$ quindi è possibile normalizzare $\uline x$ in modo che $x_k=\underset{i=1,\hdots,n}{\max}|x_i|=1$. Pertanto, la $k$-esima equazione di $A\uline x=\uline 0$ diviene: $\sum_{j=1}^na_{kj}x_j=0.$ Da questo segue che $a_{kk}\,x_k=-\sum_{j=1,j\neq k}^n a_{kj}x_j.$ Pertanto:
    \begin{equation*}
        |a_{kk}|=|a_{kk}x_k|=\left|\sum_{j=1,j\neq k}^n a_{kj}x_j\right|\leq\sum_{j=1,j\neq k}^n |a_{kj}|\cdot\underbrace{|x_j|}_{\leq 1}\leq\sum_{j=1,j\neq k}^n |a_{kj}|,
    \end{equation*}
    il che contradice l'ipotesi d.d. per righe sulla riga $k$. Se A è d.d. per colonne, il tutto è riapplicato a $A^T$, che sarà d.d. per righe (per \textbf{P1)}).
\end{itemize}

\noindent Dalle proprietà \textbf{P2)} e \textbf{P3)}, segue che:
\begin{itemize}
    \item $A$ è d.d. per righe (o colonne) $\iff\forall k=1,\hdots,n;$
    \item $A_k$ è d.d. per righe (o colonne) $\iff\forall k=1,\hdots,n;$
    \item $\det(A_k)\neq 0\iff A$ è fattorizzabile $LU.$
\end{itemize}

\paragraph{5.}\footnote{Slide 9 PDF 14.} Sia $A\in\mathbb R^{n\times n}.$ A è simmetrica e definita positiva (sdp) se:
\begin{itemize}
    \item $A=A^T$ ($A$ è simmetrica);
    \item $\forall \uline x\in\mathbb R^n\backslash\{\uline 0\}\,:\, \uline x^T A \uline x > 0$ ($A$ è definita positiva).
\end{itemize}
Per dimostrare che, se $A$ è sdp, allora $A=LU$, sarà dimostrato che:
\begin{enumerate}
    \item $\forall k=1,\hdots, n$, $A_k$, detta sottomatrice principale di $A$ di ordine $k$, è sdp;
    \item $A$ \textit{sdp} $\Rightarrow A$ è nonsingolare.
\end{enumerate}

\noindent Riguardo la 2), se $A$ non fosse \gls{singolare}, allora $\exists\,\uline x\in\mathbb R^n\backslash\{\uline 0\}\, : \, A\uline x=\uline 0\Rightarrow\uline x^T A \uline x = 0,$ contraddicendo la definita positiva di A.
Pertanto, $A$ è nonsingolare.

\noindent Riguardo la 1): 
\begin{equation}\label{eq:ACompSimm}
\forall k=1,\hdots,n:\; 
A=\left[\begin{array}{c|c}
       A_k & B\\
       \hline
       C& D
    \end{array}
    \right],\text{ con } D\in\mathbb R^{(n-k)\times (n-k)}.    
\end{equation}
Dalla simmetria di $A$, segue che 
$A^T=\left[
\begin{array}{c|c}
    A_k & C^T\\
    \hline
    B^T & D^T
\end{array}
\right]=A.$ Uguagliando i blocchi omologhi allora:
\begin{equation*}
    A_k=A_k^T,\; B=C^T,\; D=D^T\;\rightarrow A_k \text{ è simmetrica.}
\end{equation*}

\noindent Rimane da dimostrare che, $\forall \uline y\in\mathbb R^k\backslash\{\uline 0\}\,:\, \uline y^TA_k\uline y > 0.$ A questo fine, assegnato un generico $\uline y\in\mathbb R^k\backslash\{\uline 0\}$, è considerato il vettore $\uline x=
\begin{pmatrix}
    \uline y\\
    \uline 0
\end{pmatrix}\in\mathbb R^n$. Chiaramente $\uline x\neq 0$. Dato che $A$ è \textit{sdp} allora da (\ref{eq:ACompSimm}) segue che:
\begin{equation*}
    0<\uline x^T A \uline x = (\uline y^T\; \uline 0^T)\,\left[\begin{array}{c|c}
       A_k & B\\
       \hline
       C& D
    \end{array}
    \right]
    \begin{pmatrix}
        \uline y\\
        \uline 0
    \end{pmatrix} = 
    (\uline y^T\; \uline 0^T)
    \begin{bmatrix}
        A_k \uline y\\
        C \uline y
    \end{bmatrix} = \uline y^T A_k \uline y.
\end{equation*}\qed

\paragraph{6.}\footnote{Slide 11 PDF 14.}
È noto che, se $A$ è sdp, allora $A=LU$ ed è noto che la fattorizzazione $LU$ è unica. Pertanto, osservando che il fattore $U$ può essere scritto come
\begin{equation*}
	U = D\widehat{U},
\end{equation*}
con
\begin{itemize}
	\item $D$ diagonale,
	\item $\widehat{U}$ triangolare superiore a diagonale unitaria.
	\end{itemize}
	Segue che $A=LU=LD\widehat{U}$. Essendo $A=A^T$, allora
	\begin{equation*}
		A=LD\widehat{U}=(LD\widehat{U})^T=A^T.
	\end{equation*}
	Segue che
	\begin{equation*}
		LD\widehat{U}=\widehat{U}^TDL^T.
	\end{equation*}

\noindent Essendo:
\begin{itemize}
    \item $\widehat{U}^T$ triangolare inferiore a diagonale unitaria,
    \item $DL^T$ triangolare superiore,
\end{itemize}
per l'unicità della fattorizzazione $LU$ segue che $L=\widehat{U}^T$. Da questo è possibile concludere che
\begin{equation*}
	A=LDL^T.\qed
\end{equation*}

\paragraph{7.}\footnote{Slide 12 PDF 14.} Sia $||\cdot||$ una norma assegnata su vettore. È definito, data $A\in\mathbb R^{m\times n}$, la sua norma indotta dalla norma su vettore considerata: $||A||=\underset{\uline x\in\mathbb R^n, ||x||=1}{\max}||A\uline x||$.

\begin{remark}
    $||A\uline x||$ è la norma di un vettore di $\mathbb R^m.$
\end{remark}

\noindent Alcuni esempi: se $A=(a_{ij})\in\mathbb R^{m\times n},$ allora:
\begin{itemize}
    \item $||A||_1=\underset{j=1,\hdots,n}{\max}\sum_{i=1}^m|a_{ij}|;$
    \item $||A||_\infty=\underset{i=1,\hdots,m}{\max}\sum_{j=1}^n|a_{ij}|.$
\end{itemize}

\paragraph{8.}\footnote{Slide 12 PDF 14.} Sia $A\in\mathbb R^{n\times n},\;\det(A).$ Il suo numero di condizione, assegnato ad una generica norma indotta da una corrispondente norma su matrice, è definito come $\kappa(A)=||A||\cdot||A^{-1}||.$ Questo misura il condizionamento del sistema lineare $A\uline x= \uline b.$ Infatti, considerando perturbazioni $\Delta A$ e $\Delta\uline b$ su $A$ e $\uline b$, rispetivamente, è ottenuto $(A+\Delta A)(\uline x+\Delta\uline x)= \uline b+\Delta\uline b,$ con $\Delta\uline x$ perturbazione sul risultato. Vale che:
\begin{equation*}
    \frac{||\Delta\uline x||}{||\uline x||}\leq\kappa(A)\left(\frac{||\Delta\uline b||}{||\uline b||}+\frac{||\Delta A||}{||A||}\right)\qed
\end{equation*}

\paragraph{9.}\footnote{Slide 13 PDF 14.} Sia $A\in\mathbb R^{m\times n},$ con $m>n=rank(A).$ È definita \textbf{soluzione nel senso dei minimi quadrati} del sistema lineare $\boldsymbol{A\uline x=\uline b},$ il vettore $\boldsymbol{\uline x}$ \textbf{che minimizza la norma euclidea (al quadrato) del corrispondente vettore residue} $\boldsymbol{\uline r=A\uline x-\uline b}.$ \footnote{Necessario specificare la parte in grassetto affinché la risposta sia corretta.}

\noindent Il motivo della scelta della norma euclidea è dovuto al fatto che questa è invariante per moltiplicazione del vettore in argomento per una matrice ortogonale $Q$:
\begin{equation*}
    ||Q\uline v||_2^2=(Q\uline v)^T(Q\uline v)=\uline v^T \underbrace{Q^TQ}_{\footnotemark}=\uline v^T\uline v=||\uline V||_2^2.
\end{equation*}\footnotetext{Uguale ad $I$ perché $Q$ è ortogonale.}
La soluzione ai minimi quadrati di $A\uline x=\uline b$ è ottenuta osservando che, se $A\in\mathbb{m\times n},\; m>n=rank(A)$, allora esistono:
\begin{itemize}
    \item $Q\in\mathbb R^{m\times m},\; Q$ ortogonale ;
    \item $\widehat{R}\in\mathbb R^{n\times n},\;\widehat{R}$ triangolare superiore e nonsingolare,
\end{itemize}
tali che.: $A=QR,$ con $R=\begin{pmatrix}
    \widehat{R}\\
    0
\end{pmatrix}\in\mathbb R^{m\times n}.$ Per minimizzare:
\begin{equation*}
    \begin{matrix}
        ||\uline r||_2^2&=&||A\uline x-\uline b||_2^2&=&||QR\uline x-\uline b||_2^2 &=& ||Q(R\uline x - Q^T\uline b)||_2^2&\overset{\footnotemark}{=}&||R\uline x-\uline g||_2^2\\
        &\overset{\footnotemark}{=}&
        \Biggl|\Biggl|\begin{bmatrix}
            \widehat{R}\\
            0
        \end{bmatrix}\uline x
        - \begin{bmatrix}
            \uline g_1\\
            \uline g_2
        \end{bmatrix}\Biggl|\Biggl|_2^2&=&\Biggl|\Biggl|\begin{bmatrix}
            \widehat{R}\uline x -\uline g_1\\
            -\uline g_1
        \end{bmatrix}\Biggl|\Biggl|_2^2 &=& \left|\left|\widehat{R}\uline x - \uline g_1\right|\right|_2^2 + \left|\left|\uline g_2\right|\right|_2^2 &=& \left|\left|\uline g_2\right|\right|_2^2\\
        &&&&&&&=& \min!
    \end{matrix}
\end{equation*}

\addtocounter{footnote}{-1}
\footnotetext{Ponendo $\uline g=Q^T\uline b$ e sfruttando l'invarianza di $||\cdot||_2$.}

\stepcounter{footnote}
\footnotetext{$\uline g_1\in\mathbb R^n$.}

\noindent Scegliendo $\uline{x}$ come soluzione del sistema lineare $\widehat{R}\,\uline x=\uline g_1$, che esiste, ed è unica, essendo $\widehat{R}$ nonsingolare.

\paragraph{10.}\footnote{Slide 15 PDF 14.} È noto che $H\boldsymbol{\uline z}=\alpha\uline e_1\in\mathbb R^3, $ con $\alpha=\pm\left|\left|\uline z\right|\right|_2=\pm 3.$ La matrice $H$ è nella forma
\begin{equation*}
    H=I-\frac{2}{\uline v^T\uline v}\uline v\uline v^T,
\end{equation*}
con
\begin{equation*}
    \uline v=\uline z-\alpha \uline e_1=
    \begin{bmatrix}
        -1-\alpha\\
        2\\
        -2
    \end{bmatrix}=
    \begin{bmatrix}
        -1-3\\
        2\\
        -2
    \end{bmatrix}
\end{equation*}
avendo scelto \textbf{$\boldsymbol{\alpha=3}$, in modo che la prima componente di $\boldsymbol{\uline v}$ sia ottenuta sommando quantità concordi.} Segue che $\boldsymbol{H\uline z= 3\uline e_1}$. Il vettore di Householder è $\uline v=\begin{bmatrix}
    -4\\
    2\\
    -2
\end{bmatrix},$ dal quale è ottenuto che
\begin{equation*}
    H=I-\frac{2}{\uline v^T\uline v}\uline v\uline v^T=I-\frac{2}{24}\uline v \uline v^T=I-\frac{1}{12}\uline v\, \uline v^T.
\end{equation*}

\paragraph{11.}\footnote{Slide 17 PDF 14.} Sia $f\,:\,\mathbb R^m\rightarrow\mathbb R^m,$ è definito il metodo di Newton per risolvere $f(\uline x)=\uline 0$ fondamentalmente dall'iterazione seguente:
\begin{equation}\label{eq:passoFondamNewt}
    \uline x_{n+1}=\uline x_n - J_f(\uline x_n)^{-1}f(\uline x_n),\; n=0,1\hdots
\end{equation}
essendo $J_f(\uline x_n)$ la matrice Jacobiana di $f(\uline x)$ calcolata in $\uline x_n$. Nella pratica (\ref{eq:passoFondamNewt}) è implementata con l'obbiettivo di risolvere il sistema lineare $J_f(\uline x_n)\Delta \uline x_n=-f(\uline x_n)$ aggiornando $\uline x_{n+1}=\uline x_n+\Delta\uline x_n,\, n=0,1,\hdots\boldsymbol .$ Pertanto, il costo per iterazione è il seguente:
\begin{enumerate}
    \item 1 valutazione di $J_f$ e la sua fattorizzazione;
    \item 1 valutazione di $f$ e soluzione per i fattori di $J_f$;
    \item 1 axpy per aggiornare l'approssimazione corrente.
\end{enumerate}
Come criterio d'arresto è possibile utlizzare il seguente:
\begin{equation*}
    ||\Delta\uline x_n./(1+|\uline x|)||\leq tol, 
\end{equation*}
per una tolleranza $tol$ specificata.

\subsection{A.A. 2023/24}
\begin{enumerate}
	\item Scrivere una function Matlab che risolva efficientemente un sistema triangolare superiore, in modo vettoriale e per colonne.
	\item Definire la fattorizzazione $LU$ di una matrice (\gls{nonsingolare}). Dimostrare che, se una matrice nonsingolare è fattorizzabile $LU$, allora la sua fattorizzazione è unica.
	\item Sotto quali condizioni esiste la fattorizzazione $LU$ di una matrice (nonsingolare)?
	\item Definire cosa è una matrice diagonale dominante e dimostrare che è fattorizzabile $LU$.
	\item Definire una matrice simmetrica e definita positiva e dimostrare che è fattorizzabile $LU$.
	\item Sapendo che, se $A$ è s.d.p., $A$ è fattorizzabile $LU$, dimostrare che $A=LDL^T$, con $D$ matrice diagonale ad elementi positivi.
	\item Definire cosa si intende per norma indotta su matrice. Dare qualche esempio di tali norme.
	\item Definire cosa è il numero di condizionamento di una matrice. Spiegarne il significato.
	\item Definire la soluzione, nel senso dei minimi quadrati, di un sistema lineare sovradimensionato. Determinarne l'esistenza ed unicita'.
	\item Costruire la matrice di Householder relativa al vettore $\uline z = \begin{bmatrix}
		1\\
		-2\\
		-2
	\end{bmatrix}$. Quanto vale $H\uline z$?
	\item Definire il metodo di Newton per sistemi nonlineari e dettagliarne l'implementazione.
\end{enumerate}

\hrule

\paragraph{1.} Vedere Algoritmo \ref{alg:triu}.
\paragraph{Varianti sul tema:}
\begin{itemize}
	\item sistema triangolare inferiore,
	\item risoluzione dei sistemi $Ly=b$ e $Ux=y$, se abbiamo in ingresso una matrice riscritta con l'informazione dei fattori $L$ e $U$ della fattorizzazione $LU$ di $A$.
\end{itemize}

\paragraph{2.}\footnote{Teorema \ref{th:unicita_fattorizzazione_LU} Unicita' della fattorizzazione $LU$.} Sia $A\in\mathbb{R}^{n \times n},\, \det(A)\neq 0$. $A$ è fattorizzabile $LU$ se esistono:
\begin{enumerate}
	\item $L$ matrice triangolare inferiore a diagonale unitaria,
	\item $U$ matrice triangolare superiore,
\end{enumerate}
tali che $A=L\cdot U$.\\
Sia $A=LU$. Se $L_1\, U_1$ fosse un'ulteriore fattorizzazione $LU$ di $A$, si avrebbe che:
\begin{equation}\label{eq:A=L1U1}
	A=LU=L_1U_1.
\end{equation}
Tuttavia:
\begin{equation*}
	0 \neq \det(A) = \det(L\cdot U) = \overbrace{\det(L)}^{1}\cdot\det(U) = \det(U).
\end{equation*}
Similmente, si ha che $\det(U_1)\neq 0$. Pertanto, da (\ref{eq:A=L1U1}), moltiplicando a sinistra, membro a membro, per $L_1^{-1}$, si ottiene:
\begin{equation*}
	L_1^{-1}LU=\overbrace{L_1^{-1}L_1}^{I}U_1 = U_1. 
\end{equation*}
Moltiplicando a destra per $U^{-1}$ si ottiene:
\begin{equation}\label{eq:U1U-1=L1-1L}
	L_1^{-1}L=L_1^{-1} L \overbrace{UU^{-1}}^{I}=U_1U^{-1}.
\end{equation}
Osserviamo che:
\begin{enumerate}
	\item $U_1,\, U,\, U^{-1}$ sono triangolari superiori e quindi $U_1U^{-1}$ è una matrice triangolare superiore.
	\item analogamente $L,\, L_1$ e $L_1^{-1}$ sono triangolari inferiori a diagonale unitaria. Pertanto, $L_1^{-1}L$ è una matrice triangolare inferiore a diagonale unitaria.
\end{enumerate}
In conclusione, dalla (\ref{eq:U1U-1=L1-1L}) concludiamo che $U_1U^{-1}$ e $L_1^{-1}L$ sono matrici diagonali. Poiche' la diagonale di $L_1^{-1}L$ è unitaria si conclude che
\begin{equation*}
	U_1U^{-1} = I = L_1^{-1}L,
\end{equation*}
da cui si ottiene:
\begin{equation*}
	U_1 = U \quad \wedge \quad L_1 = L.
\end{equation*}
Ovvero, la fattorizzazione $LU$ è unica.

\paragraph{3.}\footnote{Teorema \ref{th:esistenza_fattorizzazione_LU} Esistenza della fattorizzazione $LU$.} Sia $A\in\mathbb{R}^{n \times n},\, \det(A)\neq 0$. Denotiamo con $A_k\in\mathbb{R}^{k\times k}$ la sua sottomatrice principale di ordine $k$. Allora:
\begin{equation*}
	A = LU \iff \det(A_k)\neq 0,\quad\forall k=1,\hdots, n.
\end{equation*}

\paragraph{4.}\footnote{Definizione (\ref{def:matrice_diagonale_dominante}) Matrice diagonale dominante, Dimostrazione Teorema \ref{th:matrice_diagonale_dominante_nonsingolare}.} Sia $A=(a_{ij})\in\mathbb{R}^{n \times n}$. Essa si dira' a diagonale dominante:
\begin{enumerate}
	\item per righe, se $|a_{ii}|>\sum_{\underset{j\neq i}{j=1}}^n |a_{ij}|,\quad 
	\forall i=1,\hdots, n$;
	\item per colonne se $|a_{jj}|>\sum_{\underset{i\neq j}{j=1}}^n|a_{ji}|,\quad \forall j=1,\hdots,n$.
\end{enumerate}
\textbf{Proprieta':}
\begin{enumerate}
	\item $A$ d.d. per righe s.se $A^T$ d.d. per colonne;
	\item $A$ d.d. s.se $\forall k=1,\hdots, n,\, A_k$ d.d., essendo $A_k\in\mathbb{R}^{k\times k}$ la sottomatrice principale di ordine $k$;
	\item $A$ d.d. allora $\det(A)\neq 0$. Infatti, supponiamo che $A$ sia diagonale dominante per righe (altrimenti considero $A^T$). Se fosse $A$ \gls{singolare} allora $\exists \uline x\in\mathbb{R}^m,\, (x_1,\hdots, x_n)^T\equiv \uline x \neq 0\,\colon\, A\uline x = \uline 0$. Poiche' questo vale per ogni multiplo scalare di $\uline x$, è possibile assumere che $x_k=1=\underset{i=1,\hdots, n}{\max}|x_i|$. Se scriviamo la $k$-esima equazione di $A\uline x=\uline 0$ è ottenuto:
	\begin{equation*}
		\begin{matrix}
			\sum_{j=1}^{n}a_{kj} x_j=0 &\Rightarrow& a_{kk}x_k = -\sum_{\underset{j\neq k}{j=1}}^{n}a_{kj} x_j && \\\\
			&\Rightarrow& |a_{kk}| = \left|\sum_{\underset{j\neq k}{j=1}}^{n}a_{kj} x_j\right| &&\\\\
			&\leq& \sum_{\underset{j\neq k}{j=1}}^{n}|a_{kj}| \underbrace{|x_j|}_{\leq 1} &\leq& \sum_{\underset{j\neq k}{j=1}}^{n}|a_{kj}|.
		\end{matrix}
	\end{equation*}
	\item $A$ d.d. $\Rightarrow\forall k=1,\hdots,n\colon A_k$ è d.d. $\Rightarrow A=LU$.
\end{enumerate}

\paragraph{5.} Sia $A\in\mathbb{R}^{n\times n}$. Diremo che $A$ è s.d.p. se:
\begin{enumerate}
	\item $A=A^T$ (ovvero $A$ è simmetrica);
	\item $\forall\uline x\in\mathbb{R}^n\backslash\{\uline 0\}\colon \uline x^T A \uline x > 0$ (ovvero $A$ è definita positiva).
\end{enumerate}
Osserviamo che:
\begin{enumerate}
	\item $A$ s.d.p. $\Rightarrow\det(A)\neq 0$. Infatti, se fosse \gls{singolare} allora $\exists\uline x\in\mathbb{R}^n\backslash\{\uline 0\}\colon A\uline x = \uline 0 \Rightarrow \uline x^T A \uline x = 0$, il che contraddice il fatto che $A$ sia s.d.p.;
	\item $A$ s.d.p. $\Rightarrow \forall k=1,\hdots, n$, data $A_k$ la sottomatrice principale di ordine $k$ di $A$, allora $A_k$ è simmetrica definita positiva. Infatti, possiamo scrivere:
	\begin{equation*}
		A=\left[
		\begin{array}{c|c}
			A_k & B\\
			\hline
			C & D
		\end{array}
		\right], \text{ con } D\in\mathbb R^{(n-k)\times (n-k)},\, A_k\in\mathbb{R}^{k\times k}
	\end{equation*}
	e poiche'
	\begin{equation*}
		A^T=A=\left[
		\begin{array}{c|c}
			A_k & C^T\\
			\hline
			B^T & D^T
		\end{array}
		\right],
	\end{equation*}
	si deduce che $A_k=A_k^T,\, D=D^T$ e $B = C^T$. Pertanto $A_k$ è simmetrica.\\
	Rimane da dimostrare che $\forall\uline y\in\mathbb{R}^k\backslash\{\uline 0\}\colon\uline y^T A_k \uline y > 0$. Infatti, se considerata
	\begin{equation*}
		\uline x = 
		\begin{bmatrix}
			\uline y\\
			\uline 0
		\end{bmatrix}\in\mathbb{R}^n, \text{ se } \uline y\neq\uline 0 \Rightarrow\uline x \neq \uline 0.
	\end{equation*}
	Pertanto, essendo $A$ s.d.p., si ottiene:
	\begin{equation*}
		0 < \uline x^T A \uline x =
		\begin{bmatrix}
			\uline y^T & \uline 0^T
		\end{bmatrix} = 
		\left[\begin{array}{c|c}
			A_k & B\\
			\hline
			C& D
		\end{array}\right]
		\begin{bmatrix}
			\uline y\\
			\uline 0
		\end{bmatrix} = 
		\begin{bmatrix}
			\uline y^T A_k & \uline y^T B
		\end{bmatrix}
		\begin{bmatrix}
			\uline y\\
			\uline 0
		\end{bmatrix} = \uline y^T A_k \uline y.
	\end{equation*}
	\item $A$ s.d.p. $\Rightarrow\forall k=1,\hdots, n\colon A_k$ s.d.p. $\Rightarrow\forall k=1,\hdots, n\colon\det(A_k)\neq 0 \Rightarrow A=LU$.
\end{enumerate}

\paragraph{6.} Sia $A$ s.d.p. allora
\begin{equation}\label{eq:A=LU=LDL}
	A = LU = L D \hat{U},
\end{equation}
con:
\begin{itemize}
	\item $D$ matrice diagonale contenente gli elementi diagonali di $U$,
	\item $\hat{U}$ triangolare superiore a diagonale unitaria.
\end{itemize}
(E' utile ricordare che $L$ è triangolare inferiore a diagonale unitaria.)\\
Poiche' $A=A^T$, dalla (\ref{eq:A=LU=LDL}) ricaviamo che
\begin{equation}\label{eq:A=U^TDL^T}
	A = (L D \hat{U})^T = \hat{U}^T D^T L^T = \hat{U}^T D L^T.
\end{equation}
Poiche':
\begin{itemize}
	\item $\hat{U}^T$ è triangolare inferiore a diagonale unitaria,
	\item $DL^T$ è triangolare superiore,
	\item la fattorizzazione $LU$ di una matrice (nonsingolare) è unica, da (\ref{eq:A=LU=LDL}) e (\ref{eq:A=U^TDL^T}), è dedotto che $L=\hat{U}^T$ e $A=LDL^T$.\\
	Se $D =
	\begin{bmatrix}
		d_1\\
		&\ddots\\
		& & d_n
	\end{bmatrix}$, abbiamo che, per un generico $i\in\{1,\hdots, n\}\colon d_i = \uline e_i^T D \uline e_i$, con $\uline e_i\in\mathbb{R}^n,\, i$-esimo versore della base canonica. Inoltre essendo $L^T$ nonsingolare, il sistema $L^T\uline x = \uline e_i$ è risolvibile e $\boldsymbol{\uline x\neq \uline 0}$. Segue che:
	\begin{equation*}
		d_i = \uline e_i^T D \uline e_i = \left(L^T\uline x\right)^T D L^T \uline x =\uline x^T \left(LDL^T\right) \uline x = \uline x^T A \uline x > 0.
	\end{equation*}
\end{itemize}


\paragraph{7.}\footnote{Vedere Sezione \ref{sssec:norme_indotte_matrice}.} Sia $||\cdot||$ una assegnata norma su vettore. Per esempio, se $\uline x\in\mathbb{R}^n$:
\begin{enumerate}
	\item $||\uline x||_\infty = \underset{i}{\max}|x_i|$,
	\item $||\uline x||_1 = \sum_{i=1}^{n}|x_i|$,
	\item $||\uline x||_2 = \sqrt{\sum_{i=1}^{n}|x_i|^2} = \uline x^T\uline x$.
\end{enumerate}

Si definisce, se $A\in\mathbb{R}^{m\times n}$, norma indotta dalla corrispondente norma su vettore,
\begin{equation*}
	||A|| = \underset{||\uline x|| = 1}{\max}||A \uline x||.
\end{equation*}

Ad esempio, dalle precedenti norme su vettore, se $A=(a_{ij})$, allora:
\begin{enumerate}
	\item $||A||_\infty = \underset{i = 1, \hdots, m}{\max} \sum_{j=1}^{n}|a_{ij}|$,
	\item $||A||_1 = \underset{j = 1,\hdots, n}{\max}\sum_{i = 1}^{n}|a_{ij}|$,
	\item $||A||_2 =\sqrt{\rho(A^T A)} \equiv \sqrt{\rho (AA^T)}$,
\end{enumerate}
essendo $\rho$ il raggio spettrale della matrice in argomento, ovvero il massimo dei moduli dei suoi autovalori.

\paragraph{Possibile variante dell'Esercizio 7.:} Proprieta' delle norme (vedere Proprieta' \ref{prop:norme_indotte_matrici}).

\paragraph{8.}\footnote{Vedere Definizione \ref{def:numero_condizionamento_matrice}.} Sia $A\in\mathbb{R}^{n\times n},\, \det(A) \neq 0$. Il numero di condizionamento di $A$ è definito da
\begin{equation*}
	\kappa(A) = ||A||\cdot||A^{-1}||,
\end{equation*}
dove $||\cdot||$ è una norma indotta su matrice. Osserviamo che
\begin{equation*}
	\kappa(A) = ||A||\cdot||A^{-1}|| \geq ||A\cdot A^{-1}|| = ||I|| = 1.
\end{equation*}

Inoltre, $A$ si dice malcondizionato se $\kappa(A)\gg 1$.

$\kappa(A)$ misura il condizionamento di un sistema lineare $A\uline x = \uline b$. Infatti, risolvendo il sistema perturbato
\begin{equation*}
	(A+\Delta A) (\uline x + \Delta\uline x) = \uline b + \Delta\uline b,
\end{equation*}
con $\Delta A\in\mathbb{R}^{n\times n}$ contenente le perturbazioni sugli elementi di $A$ e, similmente, $\Delta\uline b$ e $\Delta\uline x$, si ottiene (da (\ref{eq:condizionamento_sistema_lineare})):
\begin{equation*}
	 \frac{||\Delta \uline x||}{||\uline x||} \leq \kappa(A)\left(\frac{||\Delta\uline b||}{||\uline b||} + \frac{||\Delta A||}{||A||}\right),
\end{equation*}
dove la norma su vettore considerata è quella che induce norma su matrice.

\paragraph{9.}\footnote{Vedere Sezione \ref{ssec:sistemi_lineari_sovradimensionati}.} Sia dato il sistema lineare sovradimensionato
\begin{equation*}
	A\uline x =\uline b,\quad A\in\mathbb R^{m\times n},\; m> n = \rank(A).
\end{equation*}

Poiche' $b\in\mathbb{R}^m$, mentre \gls{range(A)} ha dimiensione $n<m$, ma soluzione in senso classico in genere non esiste. Pertanto, definendo il vettore residuo (come in (\ref{eq:vettore_residuo}))
\begin{equation}\label{eq:vettore_residuo_esercitazione}
	\uline r = A\uline x-\uline b=
	\begin{bmatrix}
		r_1\\
		r_2\\
		\vdots\\
		r_m
	\end{bmatrix},
\end{equation}
si ricerca $\uline x$ tale che $||\uline r||_2^2=\min!$. Poiche' $||\uline r||_{\boldsymbol{2}}^2=\sum_{i=1}^{m}r_i^2$, si parla di soluzione ai minimi quadrati.

Dimostriamo che esiste unica (la soluzione) $\uline x$. Preliminarmente osserviamo che:
\begin{enumerate}
	\item \footnote{Definizione \ref{th:esistenza_fattorizzazione_QR}} $A\in\mathbb R^{m\times n}$ allora
	\begin{itemize}
		\item $\exists Q\in\mathbb R^{\boldsymbol{m\times m}},$ \textbf{ortogonale};
		\item $\exists\widehat R\in\mathbb R^{\boldsymbol{n\times n}},$ \textbf{triangolare superiore e nonsingolare};
	\end{itemize}
	tale che
	\begin{equation}\label{eq:fattorizzazione_QR_esercitazione}
		\boldsymbol{A=QR=Q
			\begin{bmatrix}
				\widehat R\\
				O
			\end{bmatrix}.}
	\end{equation}
	Peranto $R\in\mathbb{R}^{m\times n}$.
	\item\footnote{Vedere (\ref{eq:norma_2_residuo_minimo}).} Se $Q\in\mathbb R^{\boldsymbol{m\times m}},\, Q^TQ=I,\, \uline v\in\mathbb{R}^m,$ allora:
	\begin{equation*}
		||Q\uline v||_2^2=(Q\uline v)^T(Q\uline v)=\uline v^T\underbrace{Q^TQ}_I \uline v = \uline v^T\uline v = ||\uline v||_2^2.
	\end{equation*}
	Cio' premesso, da (\ref{eq:vettore_residuo_esercitazione}) e (\ref{eq:fattorizzazione_QR_esercitazione}) segue che
	\begin{equation*}
		\begin{matrix}
			||\uline r||_2^2 &=& ||A\uline x-\uline b||_2^2 &=&||QR\uline x-\uline b||_2^2 &{=}& \left|\left|Q\left(R\uline x-Q^T\uline b\right)\right|\right|_2^2\\\\
			&{=}& ||R\uline x-Q^T\uline b||_2^2 &\overset{Q^T\uline b = \uline g}{=}& ||R\uline x - \uline g||_2^2 &=& 
			\left|\left|\begin{bmatrix}
				\widehat R\\
				O
			\end{bmatrix}\uline x - 
			\begin{bmatrix}
				g_1\\
				g_2
			\end{bmatrix}\right|\right|_2^2\\\\
			&=& \left|\left|\begin{bmatrix}
				\widehat R\uline x-\uline g_1\\
				-\uline g_2
			\end{bmatrix}\right|\right|_2^2 &=&
			\begin{bmatrix}
				\widehat R\uline x-\uline g_1\\
				-\uline g_2
			\end{bmatrix}^T
			\begin{bmatrix}
				\widehat R\uline x-\uline g_1\\
				-\uline g_2
			\end{bmatrix}&=& \left(\widehat R\uline x-\uline g_1\right)^T\left(\widehat R\uline x-\uline g_1\right)+\uline g_2^T\uline g_2 \\\\
			&{=}& \underbrace{\left|\left|\widehat R\uline x - \uline g_1\right|\right|_2^2}_0+||\uline g_2||_2^2 &{=}& ||\uline g_2||_2^2&=&\min !
		\end{matrix}
	\end{equation*}
	Se $\widehat{R}\uline x - g_1 = \uline 0$, ovvero $\uline x$ è soluzione del sistema lineare $\widehat{R}\uline x = g_1$, e tale soluzione esiste unica poiche' $\widehat{R}$ è singolare.
\end{enumerate}

\paragraph{10.}\footnote{Vedere Sezione \ref{sssec:esistenza_fattorizzazione_QR} (Esistenza della fattorizzazione $QR$) e} Ricerchiamo $H$, matrice ortogonale, tale che $H\uline z =
\begin{bmatrix}
	\alpha\\
	0\\
	0
\end{bmatrix}\equiv \alpha\uline e_1,\; \uline e_1 =
\begin{bmatrix}
	1\\
	0\\
	0
\end{bmatrix}.$

E' noto che:
\begin{itemize}
	\item $\alpha^2 = ||z||_2^2 = 9 \quad\Rightarrow\quad \alpha = \pm 3$,
	\item (per (\ref{eq:matrice_householder})) $H=I-\frac{2}{\uline v^T\uline v}\uline v\uline v^T,\quad \uline v\in\mathbb R^n\backslash\{\uline 0\}$,
\end{itemize}
(per (\ref{eq:vettore_householder})) $\uline v = \uline z-\alpha\uline e_1$ il corrispondente vettore di Householder: $\uline v = \begin{bmatrix}
	1-\alpha\\
	-2\\
	-2
\end{bmatrix}$.

Per ottenere una somma ben condizionata, il segno di $\alpha$ deve essere scelto opposto a quello della prima componente di $\uline z$. Pertanto
\begin{equation*}
	\uline v = 
	\begin{bmatrix}
		4\\
		-2\\
		-2
	\end{bmatrix},
\end{equation*}
ovvero $\alpha = -3$. Quindi, $H\uline z =
\begin{bmatrix}
	-3\\
	0\\
	0
\end{bmatrix}$.

\paragraph{11.}\footnote{Vedere Sezione \ref{ssec:risoluzione_sistemi_nonlineari} Risoluzione di sistemi nonlineari.} Sia $\uline f\colon\mathbb{R}^n\rightarrow \mathbb{R}^n$, con $f_1,\hdots, f_n\colon\mathbb{R}^n\rightarrow\mathbb{R}$ le sue funzioni componenti. Definiamo
\begin{equation*}
	F'(\uline x)=
	\begin{bmatrix}
		\frac{\partial f_1}{\partial x_1}(\uline x) & \hdots & \frac{\partial f_1}{\partial x_n}(\uline x)\\
		\vdots & &\vdots\\
		\frac{\partial f_n}{\partial x_1}(\uline x) & \hdots & \frac{\partial f_n(\uline x)}{\partial x_n}(\uline x)
	\end{bmatrix}\in\mathbb R^{n\times n},
\end{equation*}
la matrice Jacobiana di $\uline f(\uline x)$. Il metodo di Newton per risolvere $\uline f(\uline x)=\uline 0$ è definito dall'iterazione
\begin{equation*}
	\uline x_{n+1} = \uline x_n - F(\uline x_n)^{-1}\uline f(\uline x_n),\quad n=0,1,\hdots.
\end{equation*}
Nella pratica, si risolve il sistema lineare
\begin{equation*}
	F(\uline x_n)\Delta\uline x_n = -\uline f(\uline x_n)
\end{equation*}
e, quindi,
\begin{equation*}
	\uline x_{n+1} = \uline x_n + \Delta \uline x_n,\quad n = 0, 1, \hdots.
\end{equation*}